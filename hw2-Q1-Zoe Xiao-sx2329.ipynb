{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22b485e2",
   "metadata": {},
   "source": [
    "** The code runs bit slow, please give it some time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fbdaa6",
   "metadata": {},
   "source": [
    "Question 1 Pt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e4056598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "70c4006e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_url = \"https://press.un.org/en\"\n",
    "urls = [seed_url] \n",
    "seen = []    \n",
    "opened = []  \n",
    "withcrisis = []\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a38822be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zoe/opt/anaconda3/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "while len(urls) > 0:\n",
    "    try:\n",
    "        curr_url=urls.pop(0)\n",
    "        req = urllib.request.Request(curr_url,headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        webpage = urllib.request.urlopen(req).read()\n",
    "        opened.append(curr_url)\n",
    "\n",
    "    except Exception as ex:\n",
    "        continue  \n",
    "\n",
    "    soup = BeautifulSoup(webpage, features=\"html.parser\") \n",
    "    text = soup.get_text().lower()\n",
    "    flag = bool(soup.find('a', {'href': '/en/press-release', 'hreflang': 'en'}))\n",
    "    if \"crisis\" in text and flag : \n",
    "        counter += 1\n",
    "        with open(f\"1_{counter}.txt\", 'w', encoding='utf-8') as f:\n",
    "            f.write(soup.prettify())\n",
    "            \n",
    "        if counter >= 10:\n",
    "            break\n",
    "\n",
    "            \n",
    "    for tag in soup.find_all('a', href = True):\n",
    "        childUrl = tag['href'] \n",
    "        o_childurl = childUrl\n",
    "        childUrl = urllib.parse.urljoin(seed_url, childUrl)\n",
    "        if seed_url in childUrl and childUrl not in seen:\n",
    "            urls.append(childUrl)\n",
    "            seen.append(childUrl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9fb1a2",
   "metadata": {},
   "source": [
    "Question 1 Pt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "de7d9676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7f3b013f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_url = \"https://www.europarl.europa.eu/news/en/press-room/page/{}\"\n",
    "urls = []\n",
    "page = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4787f6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(urls) < 10:\n",
    "    curr_url = seed_url.format(page)\n",
    "    req = urllib.request.Request(curr_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    webpage = urllib.request.urlopen(req).read()\n",
    "    soup = BeautifulSoup(webpage, features=\"html.parser\")\n",
    "\n",
    "    for tag in soup.find_all('h3', class_=\"ep-a_heading ep-layout_level2\"):\n",
    "        plenary_session_tag = tag.find('span', class_=\"ep_name\", string='Plenary session')\n",
    "        text_tag = tag.find('span', class_=\"ep_name\")\n",
    "\n",
    "        if text_tag:\n",
    "            text = text_tag.get_text().strip()\n",
    "\n",
    "            if 'crisis' in text.lower() and plenary_session_tag:\n",
    "                link = tag.find('a', href=True)\n",
    "                if link:\n",
    "                    childUrl = urllib.parse.urljoin(seed_url, link['href'])\n",
    "                    \n",
    "                    child_req = urllib.request.Request(childUrl, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                    child_webpage = urllib.request.urlopen(child_req).read()\n",
    "\n",
    "                    filename = f\"2_{len(urls)+1}.txt\"\n",
    "                    with open(filename, 'w', encoding='utf-8') as file:\n",
    "                        file.write(child_webpage.decode('utf-8'))\n",
    "                    urls.append([text, childUrl])\n",
    "\n",
    "    page += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6d2845",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
